demo()
cat("a")
cat("a")
cat("a")
cat("b",
" c")
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
data(HouseVotes84, package = "mlbench")
install.packages("mlbench")
data(HouseVotes84, package = "mlbench")
model <- naiveBayes(Class ~ ., data=HouseVotes84)
install.packages("e1071")
model <- naiveBayes(Class ~ ., data=HouseVotes84)
library(e1071)
model <- naiveBayes(Class ~ ., data=HouseVotes84)
predict(model, HouseVotes84[1:10,])
HouseVotes84[1:10]
HouseVotes84[1:10,]
predict(model, HouseVotes84[1:10,])
pred <- predict(model, HouseVotes84[1:10,])
table(pred, HouseVotes84$Class)
table(pred, HouseVotes84[1:10,]$Class)
HouseVotes84[1:10,]
HouseVotes84[1:10,]$Class
pred
res <- HouseVotes84[1:10,]$Class
table(pred, res)
mydata = rad.csv("/home/bpiekarski/workspace/studia/sem_4/MOW/projekt/dane/wybrane/smsspamcollection/sample.csv")
mydata = read.csv("/home/bpiekarski/workspace/studia/sem_4/MOW/projekt/dane/wybrane/smsspamcollection/sample.csv", sep="\t")
mydata
mydata = read.table("/home/bpiekarski/workspace/studia/sem_4/MOW/projekt/dane/wybrane/smsspamcollection/sample.csv", sep="\t")
mydata
mydata[1,]
mydata[2,]
mydata[,1]
mydata = read.table("/home/bpiekarski/workspace/studia/sem_4/MOW/projekt/dane/wybrane/smsspamcollection/sample.csv", sep="\t")
mydata
mydata[1:3,]
mydata = read.table("/home/bpiekarski/workspace/studia/sem_4/MOW/projekt/dane/wybrane/smsspamcollection/a.csv", sep="\t")
mydata[1:3,]
mydata[1:20,1]
mydata = read.table("/home/bpiekarski/workspace/studia/sem_4/MOW/projekt/dane/wybrane/smsspamcollection/a.csv", sep="\t", quote="")
mydata[1,]
mydata[100,]
mydata[200,]
mydata = read.table("/home/bpiekarski/workspace/studia/sem_4/MOW/projekt/dane/wybrane/smsspamcollection/a.csv", sep="\t", quote="")
"a"+"b"
paste(c("hello", "aaa"), "b", sep=" ")
path = "/home/bpiekarski/workspace/studia/sem_4/MOW/projekt/dane/wybrane/smsspamcollection/a.csv"
pase("a", "B")
paste("a", "B")
path_data_sms = paste(path_data_root, "wybrane/smsspamcollection/a.csv", sep="/")
path_data_sms = paste(path_data_root, "wybrane/smsspamcollection/a.csv", sep="/")
a = 5
a = "s"
paste(a, "2")
sms_raw = read.table(path_data_sms, sep="\t", quote="")
path_project = "/home/bpiekarski/workspace/studia/sem_4/MOW/projekt"
path_data_root = paste(path_project,"dane", sep="/")
path_data_sms = paste(path_data_root, "wybrane/smsspamcollection/a.csv", sep="/")
path_data_sms = paste(path_data_root, "wybrane/smsspamcollection/a.csv", sep="/")
path_data_sms = paste(path_data_root, "wybrane/smsspamcollection/a.csv", sep="/")
install.packages(tm)
install.packages("tm")
install.packages("SnowballC")
0.4*7
floor(2.8)
ceil(0.1)
ceiling(0.1)
a = [1,2,3,4,5]
a = c(1,2,3,4,5)
c[2:]
c[2:-1]
c[2:3]
a[2:3]
a[2:]
a[2:-1]
a[2:NROW(a)]
1:5
library(tm)
library(SnowballC)
# setting paths
path_project <-  ".."
path_data_root  <- paste(path_project,"datasets", sep="/")
path_data_sms <- paste(path_data_root, "sms.csv", sep="/")
# setting consts
TEST_SET_PART  <- 0.2
WORD_MIN_FREQ  <- 0.2
# reading data
sms_raw <- read.table(path_data_sms
, sep="\t"
, quote=""
, stringsAsFactors=FALSE
, header=FALSE)
# setting column names
colnames(sms_raw)  <- c("type", "message")
# setting factors
sms_raw$type  <- factor(sms_raw$type)
# creating corpus
sms_corpus  <- VCorpus(VectorSource(sms_raw$message))
# tokenizaition and corpus cleaning
sms_dtm  <- DocumentTermMatrix(sms_corpus,
control = list(tolower=TRUE
, removeNumbers=TRUE
, stopwords=TRUE
, removePunctuation=TRUE
, stemming=TRUE))
# splitting data
set_count <- NROW(sms_dtm)
test_set_count  <- ceiling(set_count * TEST_SET_PART)
learn_set_count  <- set_count - test_set_count
test_idxs  <- 1:test_set_count
learn_idxs  <-  test_set_count+1:set_count
test_set  <- sms_dtm[test_idxs,]
learn_set  <- sms_dtm[learn_idxs,]
test_types <- sms_raw[test_idxs,]$type
learn_types <- sms_raw[learn_idxs,]$type
# finding frequent words
word_count  <- rowSum(as.matrix(learn_set))
word_min_count  <-  ceiling(word_count * WORD_MIN_FREQ)
frequent_words  <- findFreqTerms(learn_set, word_min_count)
word_count
library(tm)
library(SnowballC)
# setting paths
path_project <-  ".."
path_data_root  <- paste(path_project,"datasets", sep="/")
path_data_sms <- paste(path_data_root, "sms.csv", sep="/")
# setting consts
TEST_SET_PART  <- 0.2
WORD_MIN_FREQ  <- 0.2
# reading data
sms_raw <- read.table(path_data_sms
, sep="\t"
, quote=""
, stringsAsFactors=FALSE
, header=FALSE)
# setting column names
colnames(sms_raw)  <- c("type", "message")
# setting factors
sms_raw$type  <- factor(sms_raw$type)
# creating corpus
sms_corpus  <- VCorpus(VectorSource(sms_raw$message))
# tokenizaition and corpus cleaning
sms_dtm  <- DocumentTermMatrix(sms_corpus,
control = list(tolower=TRUE
, removeNumbers=TRUE
, stopwords=TRUE
, removePunctuation=TRUE
, stemming=TRUE))
# splitting data
set_count <- NROW(sms_dtm)
test_set_count  <- ceiling(set_count * TEST_SET_PART)
learn_set_count  <- set_count - test_set_count
test_idxs  <- 1:test_set_count
learn_idxs  <-  test_set_count+1:set_count
test_set  <- sms_dtm[test_idxs,]
learn_set  <- sms_dtm[learn_idxs,]
test_types <- sms_raw[test_idxs,]$type
learn_types <- sms_raw[learn_idxs,]$type
# finding frequent words
word_count  <- rowSum(as.matrix(learn_set))
word_min_count  <-  ceiling(word_count * WORD_MIN_FREQ)
frequent_words  <- findFreqTerms(learn_set, word_min_count)
word_count
library(tm)
library(SnowballC)
# setting paths
path_project <-  ".."
path_data_root  <- paste(path_project,"datasets", sep="/")
path_data_sms <- paste(path_data_root, "sms.csv", sep="/")
# setting consts
TEST_SET_PART  <- 0.2
WORD_MIN_FREQ  <- 0.2
# reading data
sms_raw <- read.table(path_data_sms
, sep="\t"
, quote=""
, stringsAsFactors=FALSE
, header=FALSE)
# setting column names
colnames(sms_raw)  <- c("type", "message")
# setting factors
sms_raw$type  <- factor(sms_raw$type)
# creating corpus
sms_corpus  <- VCorpus(VectorSource(sms_raw$message))
# tokenizaition and corpus cleaning
sms_dtm  <- DocumentTermMatrix(sms_corpus,
control = list(tolower=TRUE
, removeNumbers=TRUE
, stopwords=TRUE
, removePunctuation=TRUE
, stemming=TRUE))
# splitting data
set_count <- NROW(sms_dtm)
test_set_count  <- ceiling(set_count * TEST_SET_PART)
learn_set_count  <- set_count - test_set_count
test_idxs  <- 1:test_set_count
learn_idxs  <-  test_set_count+1:set_count
test_set  <- sms_dtm[test_idxs,]
learn_set  <- sms_dtm[learn_idxs,]
test_types <- sms_raw[test_idxs,]$type
learn_types <- sms_raw[learn_idxs,]$type
# finding frequent words
word_count  <- rowSum(as.matrix(learn_set))
word_min_count  <-  ceiling(word_count * WORD_MIN_FREQ)
frequent_words  <- findFreqTerms(learn_set, word_min_count)
word_count
source('~/workspace/studia/sem_4/MOW/projekt/my/src/prepare_data.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/prepare_data.R')
setwd('~/workspace/studia/sem_4/MOW/projekt/my/src/')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/prepare_data.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/prepare_data.R')
test_set[1:5,]
test_set[1:5,"terms"]
test_set[1:5,'terms']
a = c(1:5)
a
a.size
a.size()
size(a)
length(a)
install.packages("gmodels")
source('~/workspace/studia/sem_4/MOW/projekt/my/src/prepare_data.R')
setwd('~/workspace/studia/sem_4/MOW/projekt/my/src/')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/prepare_data.R')
colnames(test_set)
learn_types
levels(learn_types)
numeric(1)
numeric(5)
numeric("xaxa")
class(learn_types)
table(learn_types)
typetable = table(learn_types)
typetable[["ham"]]
length(learn_set)
ROWC(learn_set)
NROW(data_set)
NROW(learn_set)
NCOL(learn_set)
learn_set[1:2,1:2]
a = typetable[["ham"]]
a
table(learn_set)
View(mydata)
View(learn_set)
learn_set
learn_set[["xmas"]]
learn_set[[xmas]]
learn_set[[xxx]]
learn_set[,xmas]
learn_set[,"xmas"]
a = learn_set[,"xmas"]
header(a)
head(a)
col.names(a)
col.name(a)
row.names(a)
names(a) = c("a","b")
names$a
a$a
a[["a"]]
a[,"a"]
a
class(a)
a
a = learn_set[,"xmas"]
learn_set
a = cbind(test_types, learn_set)
length(test_set)
NROW(test_types)
NROW(learn_set)
test_types
source('~/workspace/studia/sem_4/MOW/projekt/my/src/prepare_data.R')
length(learn_types)
cbind(learn_types, learn_set)
a
a =cbind(learn_types, learn_set)
head(a)
head(a)[1]
head(a)[2]
head(a)[3]
head(a)
names(a)
View(a)
View(a)
a[learn_types==1]
a
a$learn_types==1
subset(a, learn_types==1)
b =subset(a, learn_types==1)
View(b)
View(b)
View(b)
View(a)
View(a)
View(a)
NROW(subset(a,learn_types==2))
NROW(subset(a,learn_types==1))
NROW(subset(a,learn_types=="1"))
NROW(subset(a,learn_types=="2"))
r1 = [1,2,1,2,2]
a[,"learn_types"]
c =a[,"learn_types"]
a[,"learn_types"==1]
length(a[,"learn_types"==1])
NCOL(a[,"learn_types"==1])
NROW(a[,"learn_types"==1])
ones = a[,"learn_types"==1]
a[ones,"learn_types"]
a[ones,]
a[,ones]
row1=c(1,2,1,1,2)
row2=c("a","b","a","a","a")
badanie <- data.frame(r1=row1, r2=row2)
badanie
badanie$r1
badanie$r1==1
col(badanie, r1==1)
col(badanie, "r1"==1)
badanie
a[a$learn_types==1]
a[a$learn_types==1,]
unclass(a)
a[a$about=='No',]
setNames(numeric(2), c("a", "b"))
setNames(numeric(2), c("a", "b", "C'"))
a = setNames(numeric(2), c("a", "b"))
a
a["a"]
a$a
a["b"]
a["b",]
learn_set[learn_types==1]
learn_set[learn_types==1,]
a =learn_set[learn_types==1,]
View(a)
learn_types==1
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
rm(list=ls())
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
learn_set[learn_types==1]
learn_set[learn_types==1,]
a=learn_set[learn_types==1,]
a=learn_set[,learn_types==1]
a=learn_set[learn_types==1]
which(learn_types==1)
learn_types==1
learn_types=="ham"
a=learn_set[learn_types=="ham"]
a
a=learn_set[learn_types=="ham",]
a
View(a)
View(a)
View(a)
View(a)
b=table(a)
b
NROW(a == 1)
NROW(a == "Yes")
NROW(a == "No")
aggregate(x = q,
by = list(unique.q = q$value),
FUN = length)
apply(a,2, function(x){length(which(x == "Yes"))}
;
apply(a,2, function(x){length(which(x == "Yes"))})
b = apply(a,2, function(x){length(which(x == "Yes"))})
head(a)
a2 = data.frame(c1=c("Yes","No","No"),c2=c("No","No","No"))
b2 = apply(a2,2, function(x){length(which(x == "Yes"))})
b2
b2[c1]
b2$c1
b2["c1"]
b2["c1",]
b2[,"c1"]
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
temp_v = length(which(x=="Yes"))/temp_count
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
a2 = data.frame(c1=c(0.1,0,2),c2=c(0.6,0.9))
a2 = data.frame(c1=c(0.1,0.2),c2=c(0.6,0.9))
c = data.ftame(p1=c("Yes","No"),p2=c("Yes","Yes"))
c = data.frame(p1=c("Yes","No"),p2=c("Yes","Yes"))
apply(a2,1,function(x) {})
which(c=="Yes")
c
c[1,]
which(c[1,]=="Yes")
which(c[2,]=="Yes")
sum(c(1,2))
prod(c(1,2))
prod(c())
a2
prod(a2[1,which(c[1,]=="Yes")])*prod(1-a2[1,which(c[1,]=="No")])
prod(a2[1,which(c[1,]=="Yes")])
prod(1-a2[1,which(c[1,]=="No")])
1-c(1,2,3)
prod(a2[1,which(c[1,]=="Yes")])
prod(a2[1,which(c[1,]=="No")])
a2[1,]
c[1,]
c[1,]=="No"
which(c[1,]=="No")
which(c[1,]=="Yes")
prod(a2[1,which(c[1,]=="No")])
prod(a2[1,ifelse(length(which(c[1,]=="No")))>0, which(c[1,]=="No"),c()])
prod(ifelse(length(which(c[1,]=="No")))>0, a2[1,which(c[1,]=="No")],c())
prod(ifelse(length(which(c[1,]=="No")))>0, a2[1,which(c[1,]=="No")],c(1))
prod(ifelse(length(which(c[1,]=="No"))>0, a2[1,which(c[1,]=="No")],c(1)))
a1<-c(1:5,rep(0,5))
a2<-c(1:5,10:6)
a1
a2
length(a1)
length(a2)
A<-matrix(1:9, 3,3)
A[[1]]
A
A[[1]]
A[[2]]
a1
a2
mapply(function(X,Y){
sapply(1:length(a1),function(row){ifelse(Y[row]<3,0,Y[row])*X[row]}}, X=a1,Y=a2)
mapply(function(X,Y){
sapply(1:length(a1),function(row){ifelse(Y[row]<3,0,Y[row])*X[row]})}, X=a1,Y=a2)
mapply(function(X,Y){
apply(1:length(a1),2,function(row){ifelse(Y[row]<3,0,Y[row])*X[row]})}, X=a1,Y=a2)
1:length(a1)
mapply(function(X,Y){
mapply(1:length(a1),function(row){ifelse(Y[row]<3,0,Y[row])*X[row]})}, X=a1,Y=a2)
mapply(function(X,Y){
ifelse(Y[row]<3,0,Y[row])*X[row]}, X=a1,Y=a2)
mapply(function(X,Y){
apply(1:length(a1),2,function(row){ifelse(Y[row]<3,0,Y[row])*X[row]})}, X=a1,Y=a2)
apply(1:length(a1),1,function(row){ifelse(Y[row]<3,0,Y[row])*X[row]})}, X=a1,Y=a2)
apply(1:length(a1),1,function(row){ifelse(Y[row]<3,0,Y[row])*X[row]})}, X=a1,Y=a2)
mapply(function(X,Y){
apply(1:length(a1),1,function(row){ifelse(Y[row]<3,0,Y[row])*X[row]})}, X=a1,Y=a2)
dim(1:length(a1))
for(type: row.names(model$priors)) {
prod(ifelse(length(which(c[1,]=="No"))>0, a2[1,which(c[1,]=="No")],c(1)))
a2
c
for(type: row.names(model$priors)) {
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
, 1)
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
debugSource('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
p = model$likelihood[t,]
View(p)
View(p)
View(p)
View(p)
View(p)
View(p)
View(p)
debugSource('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
source('~/workspace/studia/sem_4/MOW/projekt/my/src/bayes_custom.R')
